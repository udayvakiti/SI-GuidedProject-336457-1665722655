{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOniEuh5qxZtBYQrcJH6lPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udayvakiti/SI-GuidedProject-336457-1665722655/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsHwCqJXpGf-",
        "outputId": "648a3769-e36b-4855-b2a4-9a4f032e9ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sentiment            id                            date       query  \\\n",
            "0       \"0\"  \"1467810672\"  \"Mon Apr 06 22:19:49 PDT 2009\"  \"NO_QUERY\"   \n",
            "1       \"0\"  \"1467810917\"  \"Mon Apr 06 22:19:53 PDT 2009\"  \"NO_QUERY\"   \n",
            "2       \"0\"  \"1467811184\"  \"Mon Apr 06 22:19:57 PDT 2009\"  \"NO_QUERY\"   \n",
            "3       \"0\"  \"1467811372\"  \"Mon Apr 06 22:20:00 PDT 2009\"  \"NO_QUERY\"   \n",
            "4       \"0\"  \"1467811592\"  \"Mon Apr 06 22:20:03 PDT 2009\"  \"NO_QUERY\"   \n",
            "\n",
            "              user                                              tweet  \n",
            "0  \"scotthamilton\"  \"is upset that he can't update his Facebook by...  \n",
            "1       \"mattycus\"  \"@Kenichan I dived many times for the ball. Ma...  \n",
            "2        \"ElleCTF\"  \"my whole body feels itchy and like its on fire \"  \n",
            "3       \"joy_wolf\"                    \"@Kwesidei not the whole crew \"  \n",
            "4        \"mybirch\"                                      \"Need a hug \"  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create an empty DataFrame to store the data\n",
        "df = pd.DataFrame(columns=['sentiment', 'id', 'date', 'query', 'user', 'tweet'])\n",
        "\n",
        "# Open the CSV file\n",
        "with open(\"/content/training.csv\", 'r', encoding='latin-1') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # Exclude the problematic rows\n",
        "    for i, line in enumerate(lines):\n",
        "        if i != 35326:  # Skip row 35326\n",
        "            data = line.strip().split(\",\")\n",
        "            if len(data) == 6:  # Ensure the line has the expected number of columns\n",
        "                df.loc[len(df)] = data\n",
        "\n",
        "# Display the loaded data\n",
        "print(df.head())\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']\n",
        "\n",
        "# Preprocess the tweets\n",
        "df['preprocessed_tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "# Create a list of preprocessed tweets\n",
        "preprocessed_tweets = df['preprocessed_tweet'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the parameters\n",
        "vocab_size = 10000  # Adjust based on your dataset size\n",
        "embedding_dim = 100  # Adjust based on your choice\n",
        "max_length = 20  # Adjust based on the desired length of generated tweets\n",
        "num_epochs = 10  # Adjust based on your choice\n",
        "batch_size = 32  # Adjust based on your choice\n",
        "\n",
        "# Prepare the data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(preprocessed_tweets)\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_tweets)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Prepare the training data\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "labels = tf.keras.utils.to_categorical(padded_sequences[:, -1], num_classes=vocab_size)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length-1),\n",
        "    tf.keras.layers.LSTM(units=64),\n",
        "    tf.keras.layers.Dense(units=vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "# Generate new tweets\n",
        "seed_text = \"I love\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    # Tokenize the seed text\n",
        "    seed_sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "    padded_seed = pad_sequences(seed_sequence, maxlen=max_length-1)\n",
        "\n",
        "    # Predict the next word\n",
        "    predicted_probs = model.predict(padded_seed)[0]\n",
        "    predicted_index = tf.random.categorical(predicted_probs.reshape(1, -1), num_samples=1)[-1, 0].numpy()\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "\n",
        "    # Append the predicted word to the seed text\n",
        "    seed_text += \" \" + predicted_word\n",
        "\n",
        "print(\"Generated Tweet:\", seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCi8RYidw0h2",
        "outputId": "ade7970f-37cc-4609-9030-361ed6457cee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "850/850 [==============================] - 28s 30ms/step - loss: 7.3217 - accuracy: 0.0529\n",
            "Epoch 2/10\n",
            "850/850 [==============================] - 31s 36ms/step - loss: 6.8545 - accuracy: 0.0543\n",
            "Epoch 3/10\n",
            "850/850 [==============================] - 27s 32ms/step - loss: 6.7117 - accuracy: 0.0570\n",
            "Epoch 4/10\n",
            "850/850 [==============================] - 26s 31ms/step - loss: 6.5458 - accuracy: 0.0598\n",
            "Epoch 5/10\n",
            "850/850 [==============================] - 26s 31ms/step - loss: 6.3721 - accuracy: 0.0627\n",
            "Epoch 6/10\n",
            "850/850 [==============================] - 27s 32ms/step - loss: 6.1901 - accuracy: 0.0682\n",
            "Epoch 7/10\n",
            "850/850 [==============================] - 27s 32ms/step - loss: 6.0033 - accuracy: 0.0758\n",
            "Epoch 8/10\n",
            "850/850 [==============================] - 26s 31ms/step - loss: 5.8156 - accuracy: 0.0845\n",
            "Epoch 9/10\n",
            "850/850 [==============================] - 26s 31ms/step - loss: 5.6223 - accuracy: 0.0946\n",
            "Epoch 10/10\n",
            "850/850 [==============================] - 26s 31ms/step - loss: 5.4295 - accuracy: 0.1061\n",
            "1/1 [==============================] - 0s 337ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Generated Tweet: I love inspector haaat rehears willcarl bangin brian lighter strip bern ole\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to calculate perplexity\n",
        "def calculate_perplexity(model, test_data):\n",
        "    # Predict the probabilities of the test data\n",
        "    predictions = model.predict(test_data)\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    cross_entropy = np.log(predictions[np.arange(len(predictions)), test_data[:, -1]]) # Get the cross-entropy for the correct words\n",
        "    perplexity = np.exp(-np.mean(cross_entropy))\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Preprocess the test data\n",
        "test_sequences = tokenizer.texts_to_sequences(test_tweets)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length)\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = calculate_perplexity(model, test_sequences)\n",
        "print('Perplexity:', perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4SYFyaG3AJN",
        "outputId": "79169208-feec-494b-e6c0-3f062a38b340"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 344ms/step\n",
            "Perplexity: 19.181423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Generate tweets\n",
        "generated_tweets = [\"Generated tweet 1\", \"Generated tweet 2\", \"Generated tweet 3\"]\n",
        "reference_tweets = [\"Reference tweet 1\", \"Reference tweet 2\", \"Reference tweet 3\"]\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu_scores = []\n",
        "for reference, generated in zip(reference_tweets, generated_tweets):\n",
        "    reference_tokens = [reference.split()]\n",
        "    generated_tokens = generated.split()\n",
        "    bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "    bleu_scores.append(bleu_score)\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVk2Our3EAF",
        "outputId": "c953d279-f2c9-4362-aeb7-cdbca0581432"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.133422688662942e-154, 1.133422688662942e-154, 1.133422688662942e-154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "\n",
        "# Generate tweets\n",
        "generated_tweets = [\"Generated tweet 1\", \"Generated tweet 2\", \"Generated tweet 3\"]\n",
        "reference_tweets = [\"Reference tweet 1\", \"Reference tweet 2\", \"Reference tweet 3\"]\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(generated_tweets, reference_tweets, avg=True)\n",
        "\n",
        "# Access the average ROUGE score\n",
        "average_rouge_score = scores['rouge-l']['f']\n",
        "print(average_rouge_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBwfVga34vYP",
        "outputId": "9e1c61e7-4d67-4294-88e2-6aa3d186921a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "0.6666666616666668\n"
          ]
        }
      ]
    }
  ]
}